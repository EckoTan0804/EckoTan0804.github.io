<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Model Selection | Haobin Tan</title>
    <link>https://eckotan0804.github.io/tag/model-selection/</link>
      <atom:link href="https://eckotan0804.github.io/tag/model-selection/index.xml" rel="self" type="application/rss+xml" />
    <description>Model Selection</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>¬© 2020</copyright><lastBuildDate>Mon, 06 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://eckotan0804.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Model Selection</title>
      <link>https://eckotan0804.github.io/tag/model-selection/</link>
    </image>
    
    <item>
      <title>Bias Variance Tradeoff</title>
      <link>https://eckotan0804.github.io/notes/machine-learning/model-selection/bias-variance-tradeoff/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://eckotan0804.github.io/notes/machine-learning/model-selection/bias-variance-tradeoff/</guid>
      <description>&lt;h2 id=&#34;tldr&#34;&gt;TL;DR&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Resaon&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;th&gt;affect&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Model&amp;rsquo;s complexity ‚¨ÜÔ∏è&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Model&amp;rsquo;s complexity ‚¨áÔ∏è&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Bias&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;wrong  assumption&lt;/td&gt;
&lt;td&gt;assume a  quadratic model to be linear&lt;/td&gt;
&lt;td&gt;underfitting&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;‚¨áÔ∏è&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;‚¨ÜÔ∏è&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Variance&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;excessive  sensitivity to small variations&lt;/td&gt;
&lt;td&gt;high-degree  polynomial model&lt;/td&gt;
&lt;td&gt;overfitting&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;‚¨ÜÔ∏è&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;‚¨áÔ∏è&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Inreducible  error&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;noisy  data&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/image-20200120105846503.png&#34; alt=&#34;image-20200120105846503&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;explaination&#34;&gt;Explaination&lt;/h2&gt;
&lt;p&gt;A model‚Äôs generalization error can be expressed as the sum of three very different errors:&lt;/p&gt;
&lt;h3 id=&#34;bias&#34;&gt;Bias&lt;/h3&gt;
&lt;p&gt;This part of the generalization error is due to &lt;strong&gt;wrong assumptions&lt;/strong&gt;, such as assuming that the data is linear when it is actually quadratic.
A high-bias model is most likely to &lt;strong&gt;underfit&lt;/strong&gt; the training data.&lt;/p&gt;
&lt;h3 id=&#34;variance&#34;&gt;Variance&lt;/h3&gt;
&lt;p&gt;This part is due to the model‚Äôs &lt;strong&gt;excessive sensitivity to small variations&lt;/strong&gt; in the training data.&lt;br&gt;
A model with many degrees of freedom (such as a high-degree polynomial model) is likely to have &lt;strong&gt;high variance&lt;/strong&gt;, and thus to &lt;strong&gt;overfit&lt;/strong&gt; the training data.&lt;/p&gt;
&lt;h3 id=&#34;irreducible-error&#34;&gt;Irreducible Error&lt;/h3&gt;
&lt;p&gt;This part is due to the &lt;strong&gt;noisiness of the data&lt;/strong&gt; itself.
The only way to reduce this part of the error is to &lt;strong&gt;clean up the data&lt;/strong&gt; (e.g., fix the data sources, such as broken sensors, or detect and remove outliers).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;High bias&lt;/th&gt;
&lt;th&gt;Low bias&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;High variance&lt;/td&gt;
&lt;td&gt;something is terribly wrong! üò≠&lt;/td&gt;
&lt;td&gt;Overfitting&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Low variance&lt;/td&gt;
&lt;td&gt;Underfitting&lt;/td&gt;
&lt;td&gt;too good to be true! ü§™&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Objective Function</title>
      <link>https://eckotan0804.github.io/notes/machine-learning/model-selection/objective-function/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://eckotan0804.github.io/notes/machine-learning/model-selection/objective-function/</guid>
      <description>&lt;h2 id=&#34;how-does-the-objective-function-look-like&#34;&gt;How does the objective function look like?&lt;/h2&gt;
&lt;p&gt;Objective function:&lt;/p&gt;
&lt;p&gt;$$
\operatorname{Obj}(\Theta)= \overbrace{L(\Theta)}^{\text {Training Loss}}  + \underbrace{\Omega(\Theta)}_{\text{Regularization}}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Training loss: measures how well the model fit on training data
$$
L=\sum_{i=1}^{n} l\left(y_{i}, g_{i}\right)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Square loss:
$$
l(y_i, \hat{y}_i) = (y_i - \hat{y}_i)^2
$$&lt;/li&gt;
&lt;li&gt;Logistic loss:
$$
l(y_i, \hat{y}_i) = y_i \log(1 + e^{-\hat{y}_i}) + (1 - y_i) \log(1 + e^{\hat{y}_i})
$$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regularization: How complicated is the model?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$L_2$ norm (Ridge): $\omega(w) = \lambda |w|^2$&lt;/li&gt;
&lt;li&gt;$L_1$ norm (Lasso): $\omega(w) = \lambda |w|$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Objective Function&lt;/th&gt;
&lt;th&gt;Linear model?&lt;/th&gt;
&lt;th&gt;Loss&lt;/th&gt;
&lt;th&gt;Regularization&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Ridge&lt;/strong&gt; regression&lt;/td&gt;
&lt;td&gt;$\sum_{i=1}^{n}\left(y_{i}-w^{\top} x_{i}\right)^{2}+\lambda|w|^{2}$&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;square&lt;/td&gt;
&lt;td&gt;$L_2$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Lasso&lt;/strong&gt; regression&lt;/td&gt;
&lt;td&gt;$\sum_{i=1}^{n}\left(y_{i}-w^{\top} x_{i}\right)^{2}+\lambda|w|$&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;square&lt;/td&gt;
&lt;td&gt;$L_2$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Logistic&lt;/strong&gt; regression&lt;/td&gt;
&lt;td&gt;$\sum_{i=1}^{n}\left[y_{i} \cdot \ln \left(1+e^{-w^{\top} x_{i}}\right)+\left(1-y_{i}\right) \cdot \ln \left(1+e^{w^{\top} x_{i}}\right)\right]+\lambda|w|^{2}$&lt;/td&gt;
&lt;td&gt;‚úÖ&lt;/td&gt;
&lt;td&gt;logistic&lt;/td&gt;
&lt;td&gt;$L_1$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;why-do-we-want-to-contain-two-component-in-the-objective&#34;&gt;Why do we want to contain two component in the objective?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimizing training loss encourages predictive models&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Fitting well in training data at least get you close to training data which is hopefully close to the underlying distribution&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimizing regularization encourages simple models&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Simpler models tends to have smaller variance in future predictions, making prediction&lt;/em&gt; &lt;em&gt;stable&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
